{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirc_path = '/home/habjan.e/'\n",
    "\n",
    "import sys\n",
    "sys.path.append(dirc_path + \"TNG/TNG_cluster_dynamics\")\n",
    "import TNG_DA\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "from IPython.display import display, Markdown\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "sys.path.append(os.getcwd())\n",
    "from training_structure import train_model, predict\n",
    "\n",
    "import jraph\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix_jax = '_testing'\n",
    "\n",
    "save_path = os.getcwd() + '/GNN_models/gnn_model_params' + suffix_jax + '.pkl'\n",
    "\n",
    "with open(save_path, 'rb') as f:\n",
    "    loaded_params = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(os.getcwd())\n",
    "from gnn import GraphConvNet\n",
    "model = GraphConvNet(latent_size = 128, \n",
    "                         hidden_size = 256, \n",
    "                         num_mlp_layers = 3, \n",
    "                         message_passing_steps = 5, \n",
    "                         skip_connections = True,\n",
    "                         edge_skip_connections = True,\n",
    "                         norm = \"pair\", \n",
    "                         attention = True,\n",
    "                         shared_weights = False,\n",
    "                         relative_updates = False,\n",
    "                         output_dim = 3,\n",
    "                         dropout_rate = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions using the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_file_path = \"/projects/mccleary_group/habjan.e/TNG/Data/GNN_SBI_data/graph_data\"\n",
    "\n",
    "#pred_train, tgt_train, mask_train = predict(model = model, params = loaded_params, data_dir = batch_file_path, data_prefix = 'train')\n",
    "#pred_test, tgt_test, mask_test = predict(model = model, params = loaded_params, data_dir = batch_file_path, data_prefix = 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick a cluster to look at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_ind = 3\n",
    "\n",
    "pos, vel, groups, subhalo_masses, h, halo_mass = TNG_DA.get_cluster_props(cluster_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some functions that will be used to make classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_SIZE = 128\n",
    "KNN_K = 16\n",
    "\n",
    "def make_graph(nodes_np: np.ndarray) -> jraph.GraphsTuple:\n",
    "    \"\"\"Convert (N, 3) numpy array -> GraphsTuple.\"\"\"\n",
    "\n",
    "    nodes = jnp.asarray(nodes_np, dtype=jnp.float32)\n",
    "    N   = nodes.shape[0]\n",
    "\n",
    "    # Pair-wise calculation of x, y, v_z\n",
    "    diffs = nodes[:, None, :] - nodes[None, :, :]\n",
    "    d2 = jnp.sum(diffs ** 2, axis=-1)\n",
    "    d2 = d2 + jnp.eye(N) * 1e9\n",
    "    knn_idx = jnp.argsort(d2, axis=1)[:, :KNN_K]\n",
    "\n",
    "    senders = jnp.repeat(jnp.arange(N, dtype=jnp.int32), KNN_K)\n",
    "    receivers = knn_idx.reshape(-1).astype(jnp.int32)\n",
    "\n",
    "    src = nodes[senders]\n",
    "    dst = nodes[receivers]\n",
    "    rel = dst - src\n",
    "    dist = jnp.linalg.norm(rel, axis=-1, keepdims=True)\n",
    "    edges = jnp.concatenate([rel, dist], axis=-1)\n",
    "\n",
    "    dummy_globals = jnp.zeros((1, LATENT_SIZE), dtype=jnp.float32)\n",
    "\n",
    "    return jraph.GraphsTuple(\n",
    "        nodes=nodes,             \n",
    "        edges=edges,\n",
    "        senders=senders,\n",
    "        receivers=receivers,\n",
    "        n_node=jnp.array([N], dtype=jnp.int32),\n",
    "        n_edge=jnp.array([edges.shape[0]],  dtype=jnp.int32),\n",
    "        globals=dummy_globals\n",
    "    )\n",
    "\n",
    "def prediction(model, params, in_graph):\n",
    "    \"\"\"\n",
    "    Make predictions with a trained model.\n",
    "    \"\"\"\n",
    "        \n",
    "    preds = model.apply({'params': params}, in_graph, deterministic = True)\n",
    "\n",
    "    return preds.nodes #jnp.concatenate(preds.nodes, axis=0).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a graph for a single cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mean = np.nanmean(pos[:, 0])\n",
    "y_mean = np.nanmean(pos[:, 1])\n",
    "vz_mean = np.nanmean(vel[:, 2])\n",
    "\n",
    "x_std = np.nanstd(pos[:, 0])\n",
    "y_std = np.nanstd(pos[:, 1])\n",
    "vz_std = np.nanstd(vel[:, 2])\n",
    "\n",
    "obs_arr = np.array([(pos[:, 0] - x_mean) / x_std, (pos[:, 1] - y_mean) / y_std, (vel[:, 2] - vz_mean) / vz_std]).T\n",
    "\n",
    "cl_graph = make_graph(obs_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on this single cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = prediction(model = model, params = loaded_params, in_graph = cl_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare true TNG positions/velocities with predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(16, 4), gridspec_kw={'wspace': 0.3})\n",
    "\n",
    "pos_mean = np.mean([x_mean, y_mean])\n",
    "pos_std = np.mean([x_std, y_std])\n",
    "\n",
    "one_one = np.linspace(-10000, 10000, 100)\n",
    "\n",
    "# Plot into each subplot\n",
    "axs[0].scatter(pos[:, 2], (preds[:, 0] * pos_std) + pos_mean, c='k', s=10)\n",
    "axs[0].plot(one_one, one_one, c='k', linestyle='--')\n",
    "axs[0].set_xlabel('TNG galaxy z-position [kpc]', fontsize = 17.5)\n",
    "axs[0].set_ylabel('GNN z-position [kpc]', fontsize = 17.5)\n",
    "lims = np.concatenate([pos[:, 2], (preds[:, 0] * pos_std) + pos_mean])\n",
    "axs[0].set_xlim(np.min(lims)*1.1, np.max(lims)*1.1)\n",
    "axs[0].set_ylim(np.min(lims)*1.1, np.max(lims)*1.1)\n",
    "\n",
    "axs[1].scatter(vel[:, 0], (preds[:, 1] * vz_std) + vz_mean, c='k', s=10)\n",
    "axs[1].plot(one_one, one_one, c='k', linestyle='--')\n",
    "axs[1].set_xlabel('TNG galaxy x-velocity [$km s^{-1}$]', fontsize = 17.5)\n",
    "axs[1].set_ylabel('GNN x-velocity [$km s^{-1}$]', fontsize = 17.5)\n",
    "lims = np.concatenate([vel[:, 0], (preds[:, 1] * vz_std) + vz_mean])\n",
    "axs[1].set_xlim(np.min(lims)*1.1, np.max(lims)*1.1)\n",
    "axs[1].set_ylim(np.min(lims)*1.1, np.max(lims)*1.1)\n",
    "\n",
    "axs[2].scatter(vel[:, 1], (preds[:, 2] * vz_std) + vz_mean, c='k', s=10)\n",
    "axs[2].plot(one_one, one_one, c='k', linestyle='--')\n",
    "axs[2].set_xlabel('TNG galaxy y-velocity [$km s^{-1}$]', fontsize = 17.5)\n",
    "axs[2].set_ylabel('GNN y-velocity [$km s^{-1}$]', fontsize = 17.5)\n",
    "lims = np.concatenate([vel[:, 1], (preds[:, 2] * vz_std) + vz_mean])\n",
    "axs[2].set_xlim(np.min(lims)*1.1, np.max(lims)*1.1)\n",
    "axs[2].set_ylim(np.min(lims)*1.1, np.max(lims)*1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import loss arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/habjan.e/TNG/Sandbox_notebooks/phase_space_recon/Loss_arrays/'\n",
    "\n",
    "test_loss = np.load(data_path + 'test_loss' + suffix_jax + '.npy')\n",
    "train_loss = np.load(data_path + 'train_loss' + suffix_jax + '.npy')\n",
    "epochs = np.arange(1, len(test_loss) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4), gridspec_kw={'wspace': 0.2})\n",
    "\n",
    "\n",
    "axs[0].plot(epochs, train_loss, color = 'blue', label = 'Training Loss')\n",
    "axs[1].plot(epochs, test_loss, color = 'red', label = 'Validation Loss')\n",
    "\n",
    "axs[0].set_xlabel('Epoch', fontsize = 20)\n",
    "axs[1].set_xlabel('Epoch', fontsize = 20)\n",
    "axs[0].set_ylabel(r'MSE Loss', fontsize = 20)\n",
    "\n",
    "#axs[0].set_yscale('symlog', linthresh=1)\n",
    "#axs[1].set_yscale('symlog', linthresh=1)\n",
    "\n",
    "for ax in axs:\n",
    "    fmt = ScalarFormatter(useMathText=True)\n",
    "    fmt.set_scientific(True)\n",
    "    fmt.set_powerlimits((0, 0))\n",
    "    fmt.set_useOffset(False)\n",
    "    ax.yaxis.set_major_formatter(fmt)\n",
    "\n",
    "axs[0].legend()\n",
    "axs[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import a BAHAMAS cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "data_path = \"/projects/mccleary_group/habjan.e/TNG/Data/GNN_SBI_data/\"\n",
    "train_file = \"GNN_data_train.h5\"\n",
    "test_file = \"GNN_data_test.h5\"\n",
    "\n",
    "key = '000256'  # sample id, zero-padded to 6 digits\n",
    "\n",
    "with h5py.File(data_path + train_file, \"r\") as f:\n",
    "\n",
    "    print(f.keys())\n",
    "\n",
    "    grp = f[key]  # this is an h5py Group\n",
    "\n",
    "    # DATASETS (arrays) -> use [:] to read\n",
    "    nodes   = grp[\"padded_nodes\"][:]\n",
    "    mask    = grp[\"node_mask\"][:]\n",
    "    targets = grp[\"padded_targets\"][:]\n",
    "\n",
    "    projection_vector = grp[\"projection_vector\"][:]\n",
    "    x_position = grp[\"x_position\"][:]\n",
    "    y_position = grp[\"y_position\"][:]\n",
    "    z_position = grp[\"z_position\"][:]\n",
    "    x_velocity = grp[\"x_velocity\"][:]\n",
    "    y_velocity = grp[\"y_velocity\"][:]\n",
    "    z_velocity = grp[\"z_velocity\"][:]\n",
    "    # (optionally) subhalo masses if you need them\n",
    "    # subhalo_masses = grp[\"subhalo_masses\"][:]\n",
    "\n",
    "    # ATTRIBUTES (scalars/metadata) -> use .attrs[]\n",
    "    sim          = grp.attrs[\"simulation\"]\n",
    "    cluster_idx  = grp.attrs[\"cluster_index\"]\n",
    "    halo_mass    = grp.attrs[\"cluster_mass\"]\n",
    "\n",
    "    x_ro_mean  = grp.attrs[\"x_position_mean\"]\n",
    "    y_ro_mean  = grp.attrs[\"y_position_mean\"]\n",
    "    z_ro_mean  = grp.attrs[\"z_position_mean\"]\n",
    "    vx_ro_mean = grp.attrs[\"x_velocity_mean\"]\n",
    "    vy_ro_mean = grp.attrs[\"y_velocity_mean\"]\n",
    "    vz_ro_mean = grp.attrs[\"z_velocity_mean\"]\n",
    "\n",
    "    x_ro_std  = grp.attrs[\"x_position_std\"]\n",
    "    y_ro_std  = grp.attrs[\"y_position_std\"]\n",
    "    z_ro_std  = grp.attrs[\"z_position_std\"]\n",
    "    vx_ro_std = grp.attrs[\"x_velocity_std\"]\n",
    "    vy_ro_std = grp.attrs[\"y_velocity_std\"]\n",
    "    vz_ro_std = grp.attrs[\"z_velocity_std\"]\n",
    "\n",
    "    # If simulation comes back as bytes, decode it:\n",
    "    if isinstance(sim, (bytes, np.bytes_)):\n",
    "        sim = sim.decode(\"utf-8\")\n",
    "\n",
    "\n",
    "print(sim, cluster_idx, np.log10(halo_mass))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_graph = make_graph(nodes)\n",
    "\n",
    "preds = prediction(model = model, params = loaded_params, in_graph = cl_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(16, 4), gridspec_kw={'wspace': 0.3})\n",
    "\n",
    "pos_mean = np.mean([x_mean, y_mean])\n",
    "pos_std = np.mean([x_std, y_std])\n",
    "\n",
    "one_one = np.linspace(-5000, 5000, 100)\n",
    "\n",
    "# Plot into each subplot\n",
    "axs[0].scatter(targets[mask, 0], preds[mask, 0], c='k', s=10)\n",
    "axs[0].plot(one_one, one_one, c='k', linestyle='--')\n",
    "axs[0].set_xlabel('TNG galaxy z-position', fontsize = 17.5)\n",
    "axs[0].set_ylabel('GNN z-position', fontsize = 17.5)\n",
    "lims = np.concatenate([targets[mask, 0], preds[mask, 0]])\n",
    "axs[0].set_xlim(np.min(lims)*1.1, np.max(lims)*1.1)\n",
    "axs[0].set_ylim(np.min(lims)*1.1, np.max(lims)*1.1)\n",
    "\n",
    "axs[1].scatter(targets[mask, 1], preds[mask, 1], c='k', s=10)\n",
    "axs[1].plot(one_one, one_one, c='k', linestyle='--')\n",
    "axs[1].set_xlabel('TNG galaxy x-velocity', fontsize = 17.5)\n",
    "axs[1].set_ylabel('GNN x-velocity', fontsize = 17.5)\n",
    "lims = np.concatenate([targets[mask, 1], preds[mask, 1]])\n",
    "axs[1].set_xlim(np.min(lims)*1.1, np.max(lims)*1.1)\n",
    "axs[1].set_ylim(np.min(lims)*1.1, np.max(lims)*1.1)\n",
    "\n",
    "axs[2].scatter(targets[mask, 2], preds[mask, 2], c='k', s=10)\n",
    "axs[2].plot(one_one, one_one, c='k', linestyle='--')\n",
    "axs[2].set_xlabel('TNG galaxy y-velocity', fontsize = 17.5)\n",
    "axs[2].set_ylabel('GNN y-velocity', fontsize = 17.5)\n",
    "lims = np.concatenate([targets[mask, 2], preds[mask, 2]])\n",
    "axs[2].set_xlim(np.min(lims)*1.1, np.max(lims)*1.1)\n",
    "axs[2].set_ylim(np.min(lims)*1.1, np.max(lims)*1.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
